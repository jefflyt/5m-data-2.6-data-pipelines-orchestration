{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a238f108",
   "metadata": {},
   "source": [
    "# Lesson Notes\n",
    "\n",
    "This notebook collects and organizes all user questions related to this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941d27",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "This lesson teaches an end-to-end ELT ingestion and orchestration workflow using Meltano, BigQuery, dbt, and Dagster. You'll create Meltano projects to extract data from GitHub and a Supabase Postgres database, test extraction to a local JSON target, and then load the data into BigQuery as the data warehouse. After ingestion, you'll scaffold a dbt project to declare sources and build models that transform the raw tables into materialized analytics tables. Finally, you'll scaffold a Dagster project, implement two Software-Defined Assets (one to fetch pandas releases and one to compute summary statistics), register a job and a schedule, and configure an I/O manager so assets are persisted and visible in the Dagster UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310873e",
   "metadata": {},
   "source": [
    "## Syntax Summary\n",
    "\n",
    "| Syntax / Token | Where seen | Purpose / Usage |\n",
    "|---|---:|---|\n",
    "| `meltano init <name>` | Meltano CLI | Create a new Meltano project scaffold |\n",
    "| `meltano add extractor <tap-name>` | Meltano CLI | Add a data extractor (tap) to the project |\n",
    "| `meltano add loader <target-name>` | Meltano CLI | Add a loader (target) to write extracted data |\n",
    "| `meltano config <plugin> set --interactive` | Meltano CLI | Interactively set plugin configuration (adds to `meltano.yml` and secrets to `.env`) |\n",
    "| `meltano select <tap> <entity> <field>` | Meltano CLI | Choose which entities/attributes the tap should extract |\n",
    "| `meltano run <tap> <target>` | Meltano CLI | Execute extract -> load pipeline |\n",
    "| `dbt init <project>` | dbt CLI | Scaffold a new dbt project |\n",
    "| `dbt debug` / `dbt run` / `dbt clean` | dbt CLI | Validate connection, run models, clean artifacts |\n",
    "| `profiles.yml` (YAML) | dbt config | Configure dbt connection targets (e.g., BigQuery service account) |\n",
    "| `.env` with `GITHUB_TOKEN` | env file | Store secrets for local runs (e.g., GitHub personal access token) |\n",
    "| `@asset` decorator | Dagster (Python) | Define a Software-Defined Asset function that returns materialized data |\n",
    "| `context.add_output_metadata` | Dagster (Python) | Attach metadata (previews, counts, images) to assets for the UI |\n",
    "| `define_asset_job(...)` / `ScheduleDefinition(...)` | Dagster API | Create a job to materialize assets and a schedule to run the job periodically |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69c01b",
   "metadata": {},
   "source": [
    "### ELT Flow (compact)\n",
    "\n",
    "1. Extract — pull raw data from sources (APIs, databases, files).\n",
    "2. Load — store the raw data directly into the data warehouse (e.g., BigQuery).\n",
    "3. Transform — run SQL/dbt models inside the warehouse to produce analytics-ready tables.\n",
    "\n",
    "```\n",
    "Sources (APIs, DBs, Files)\n",
    "        |\n",
    "        v\n",
    "     Extract (taps)\n",
    "        |\n",
    "        v\n",
    "     Load (warehouse: BigQuery, Snowflake)\n",
    "        |\n",
    "        v\n",
    "    Transform (dbt / SQL inside warehouse)\n",
    "        |\n",
    "        v\n",
    "   Analytics-ready tables / dashboards\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
